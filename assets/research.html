<html><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    body,td,th,tr,p {
    font-family: Helvetica;
    font-size: 15px;
    line-height: 1.4;
    }
    strong {
    font-family: Helvetica;
    font-size: 16px;
    }
    heading {
    font-family: Helvetica;
    font-size: 22px;
    }
    heading2 {
    font-family: Helvetica;
    font-size: 16px;
    }
    papertitle {
    font-family: Helvetica;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: Helvetica;
    font-size: 32px;
    }
/*    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }*/

img{
    display: block;
    margin: 0 auto;
    height: auto;
}

  </style>

  <link href="./_files/css" rel="stylesheet" type="text/css">

<!--
<style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
html {
  -webkit-filter: brightness(110%) contrast(90%) grayscale(20%) sepia(10%) !important;
}

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>
-->

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

  <table width=840 align="center" >
  <tr><td colspan="2">

  <p><b><font size=+1> Exploration for RL </font></b>

  <p><b>References </b></font>

  <p><b> <a href="https://arxiv.org/abs/2001.00119">Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning</a> </b> <br />
  <font color=#f10000>Simone Parisi</font>, Davide Tateo, Maximilian Hensel, Carlo D'Eramo, Jan Peters, Joni Pajarinen <br />
  Algorithms, 2022

  <p><b> <a href="https://arxiv.org/abs/2111.13119">Interesting Object, Curious Agent: Learning Task-Agnostic Exploration</a> </b> <br />
  <font color=#f10000>Simone Parisi</font>, Victoria Dean, Deepak Pathak, Abhinav Gupta <br />
  Neural Information Processing Systems (<u>NeurIPS</u>), 2021

  <hr>

  <p><b><font size=+1> Learning and Transfer of State Representations </font></b>
  <p><b><font size=+0> Towards a Near-Universal Representation for RL </font></b>

  <img src="icml2022.png" width="75%" style="border-radius:0px" >

  In computer vision and natural language processing, recent advances have allowed to exploit massive amounts of data to pre-train perception models.
  These models can be successfully used "off-the-shelf" to solve many different downstream applications without any further training.
  On the contrary, in RL many algorithms still follow a "tabula-rasa" paradigm where the agent performs millions or even billions of in-domain
  interactions with the environment to learn task-specific visuo-motor policies from scratch.

  <center><i>
  Can we instead train a single near-universal vision model --a model pre-trained entirely on out-of-domain data that works for nearly any RL task?
  </i></center>
  <br>

  In a preliminary work presented at ICML, we investigated how well-known pre-trained vision models perform when used as perception module for control
  policies in Habitat, DeepMind Control Suite, Adroit Dexterious Manipulation, and Franka Kitchen.
  We took a step back and asked fundamental questions about representations and control.
  Are off-the-shelf supervised models better than self-supervised models?
  What kind of invariances are relevant for the perception module of the control policy?
  Is the feature hierarchy of the vision layers important for control?
  <br>
  By investigating these questions, we succeeded at making a single off-the-shelf vision model
  --trained on out-of-domain datasets-- to be competitive with or even outperform ground-truth
  features on all the four control domains.
  As efficient compact state features are hard to estimate in unstructured real-world environments
  and the agent needs to rely on raw vision input,
  our model can be extremely beneficial by
  dramatically reducing the data requirement and improving the policy performance.

  <p><b>References </b></font>

  <p><b> <a href="https://arxiv.org/abs/2203.03580">The (Un)Surprising Effectiveness of Pre-Trained Vision Models for Control</a> </b> <br />
  <font color=#f10000>Simone Parisi*</font>, Aravind Rajeswaran*, Senthil Purushwalkam, Abhinav Gupta <br />
  International Conference on Machine Learning (<u>ICML</u>), 2022

  <hr>

  <p><b><font size=+1> Actor-Critic </font></b>

  <p><b>References </b></font>

  <p><b> <a href="https://arxiv.org/abs/1812.08288">TD-Regularized Actor-Critic Methods</a> </b> <br />
  <font color=#f10000>Simone Parisi</font>, Voot Tangkaratt, Jan Peters, Mohammad Emtiyaz Khan <br />
  Machine Learning, 2019

  <hr>

  <p><b><font size=+1> Real-World RL </font></b>

  When we design RL algorithms,
  While there has been a variety of publications on the
    achievements of real robot learning, no systematic comparison of learned skills against manually programmed solutions
    has been performed so far. Conducting such a systematic
    evaluation is a challenging proposition for several reasons.
    First, it is non-trivial to find a suitable measure to compare
    hand-crafted and learned policies. By design all learned
    policies try to optimize a given reward function. However,
    using this reward function as comparison measure would
    introduce a bias, as there is no guarantee that the manual
    program maximizes the same reward function. Secondly,
    manually designing a proficient program to successfully
    solve a robotic task requires deep insights into both the
    platform and the problem itself. In this paper, we propose
    a task setup that addresses these problems and allows us to
    present a systematic and thorough evaluation of state-of-theart approaches in real robot learning

  <p><b>References </b></font>

  <p><b> <a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Member/PubSimoneParisi/parisi_iros_2015.pdf">Reinforcement Learning vs Human Programming in Tetherball Robot Games</a> </b> <br />
  <font color=#f10000>Simone Parisi</font>, Hany Abdulsamad, Alexandros Paraschos, Christian Daniel, Jan Peters <br />
  International Conference on Intelligent Robots and Systems (<u>IROS</u>), 2015

  <hr>

  <p><b><font size=+1> Multi-Objective RL </font></b>

  <p><b>References </b></font>

  <p><b> <a href="https://jair.org/index.php/jair/article/view/11026">Multi-objective Reinforcement Learning through Continuous Pareto Manifold Approximation</a> </b> <br />
  <font color=#f10000>Simone Parisi</font>, Matteo Pirotta, Marcello Restelli <br />
  Journal of Artificial Intelligence Research (<u>JAIR</u>), 2016


  </td></tr>
  <hr>
  </table>


</body></html>
