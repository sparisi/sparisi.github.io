<html><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    body,td,th,tr,p {
    font-family: Helvetica;
    font-size: 15px;
    line-height: 1.4;
    }
    strong {
    font-family: Helvetica;
    font-size: 16px;
    }
    heading {
    font-family: Helvetica;
    font-size: 22px;
    }
    heading2 {
    font-family: Helvetica;
    font-size: 16px;
    }
    papertitle {
    font-family: Helvetica;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: Helvetica;
    font-size: 32px;
    }
/*    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }*/

img{
    display: block;
    margin: 0 auto;
    height: auto;
}

  </style>

  <link href="./_files/css" rel="stylesheet" type="text/css">

<!--
<style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
html {
  -webkit-filter: brightness(110%) contrast(90%) grayscale(20%) sepia(10%) !important;
}

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>
-->

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

<head>
  <title>Research - Simone Parisi</title>
  <base target="_blank">
  <!-- <body bgcolor=#f2f2f2> -->
  <!-- <body bgcolor=#F5F3EE> -->
  <!-- <body bgcolor=#ffffff> -->
</head>

  <table width=840 align="center" >
  <tr><td colspan="2">

  <p><b><font size=+1> Monitored Markov Decision Processes <br></font></b></p>
  <img src="mon_mdps.jpg" width="30%" style="border-radius:0px; display: inline; float: left; margin: 0 16 0 0 " >
  <br>

  In RL, an agent learns to perform a task by interacting with an environment and receiving rewards for
  its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. 
  Consider the situation where the agent is tasked with household chores, and the quality of its behavior is provided through rewards from the homeowner and smart sensors. 
  What if the reward is not always observable, e.g., if the owner is be present or the sensors are malfunctioning? 
  In such situations, the agent should not interpret the lack of reward as meaning that all behavior is equally desirable. 
  Neither should it think that avoiding monitoring or intentionally damaging sensors is an effective way to avoid negative feedback. 
  Further, the agent may need to reason about how to seek the most useful feedback, such as planning exploratory actions when the owner is home or in well-monitored rooms. 

  <p>
  <center><i>
  In other words, there are cases where the environment generates rewards in response to the
  agent's actions but the agent cannot observe them. How should the agent behave in such situations? 
  How can it learn to seek rewards — even if their observability may not be fully under its control?
  </i></center>
  </p>

  <a href="https://arxiv.org/abs/2203.03580">In a seminal work presented at AAMAS</a>, we formalized
  a novel but general RL framework — <i>Monitored MDPs</i> — where the agent cannot
  always observe rewards. Instead, the agent observes <i>proxy rewards</i> provided by a separate MDP the — <i>monitor</i>.
  We discussed the theoretical and practical consequences
  of this setting, showed challenges raised even in toy environments, and proposed
  algorithms to begin to tackle this novel setting. This paper introduced a
  powerful new formalism that encompasses both new and existing problems, and laid
  the foundation for future research.

  <p><b>References </b></font></p>

  <p>
  <b> <a href="https://arxiv.org/abs/2001.00119">Monitored Markov Decision Processes</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Montaser Mohammedalamen, Alireza Kazemipour, Matthew E. Taylor, Michael Bowling <br>
  International Conference on Autonomous Agents and Multiagent Systems (<u>AAMAS</u>), 2024
  </p>

  <hr>

  <p><b><font size=+1> Learning and Transfer of Exploration Policies </font></b></p>
  <img src="teaser_nips.jpg" width="18%" style="border-radius:0px; display: inline; float: left; margin: 0 16 0 0 " >
  <img src="exploration_rl.jpg" width="40%" style="border-radius:0px; display: inline; float: left; margin: 0 11 0 0" >

  Classic RL exploration is task-driven (the extrinsic reward is the main drive of exploration)
  or makes use of myopic intrinsic rewards (they depends the most promising short-term actions).
  Furthermore, learning follows a tabula-rasa approach — the agent learns from scratch,
  assuming isolated environments and no prior knowledge or experience.

  <br clear="all" />

  <p>
  <center><i>
  How can we formulate deep long-term exploration policies?
  Can we learn to explore environments before training the agent to solve tasks?
  How can we mimic the human behavior — to explore multiple environment in a lifelong process,
  driven by just the inherently interestingness of the world?
  </i></center>
  </p>

  <p>
  <a href="https://arxiv.org/abs/2001.00119">In a preliminary work published in Algorithms</a>,
  we presented a novel approach that (1) plans exploration actions far into the future by using a long-term visitation
  count, and (2) decouples exploration and exploitation by learning a separate function assessing
  the exploration value of the actions.
  </p>

  <p>
  <a href="https://openreview.net/pdf?id=knKJgksd7kA">In our NeurIPS paper</a>,
  we proposed a novel framework to pre-train and transfer exploration in a task-agnostic manner.
  The agent first learns to explore across many environments, driven only by a novel
  intrinsic rewards and without any extrinsic goal.
  Later on, the agent transfers the learned exploration policy to better explore new environments when solving tasks.
  The key idea of our framework is that there are two components of exploration:
  (1) an agent-centric component encouraging exploration of unseen parts of the environment based on an agent's belief;
  (2) an environment-centric component encouraging exploration of inherently interesting objects.
  </p>

  The results of our approach were extremely promising, and open several avenues of research.
  Is there a universal formulation of intrinsic curiosity and interestingness?
  How important are state representation to learn / transfer exploration policies?
  Can we use out-of-domain data — e.g., collected from the internet — to train exploration policy?



  <p><b>References </b></font></p>

  <p>
  <b> <a href="https://arxiv.org/abs/2001.00119">Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Davide Tateo, Maximilian Hensel, Carlo D'Eramo, Jan Peters, Joni Pajarinen <br>
  Algorithms, 2022
  </p>

  <p>
  <b> <a href="https://arxiv.org/abs/2111.13119">Interesting Object, Curious Agent: Learning Task-Agnostic Exploration</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Victoria Dean, Deepak Pathak, Abhinav Gupta <br>
  Neural Information Processing Systems (<u>NeurIPS</u>), 2021
  </p>

  <hr>

  <p><b><font size=+1> Learning and Transfer of State Representations <br></font></b></p>
  <img src="PVR-Paradigm-2.jpg" width="75%" style="border-radius:0px" >
  <br>

  In computer vision and natural language processing, recent advances have allowed to exploit massive amounts of data to pre-train perception models.
  These models can be successfully used "off-the-shelf" to solve many different downstream applications without any further training.
  On the contrary, in RL many algorithms still follow a "tabula-rasa" paradigm where the agent performs millions or even billions of in-domain
  interactions with the environment to learn task-specific visuo-motor policies from scratch.

  <p>
  <center><i>
  Can we instead train a single near-universal vision model — a model pre-trained entirely on out-of-domain data that works for nearly any RL task?
  </i></center>
  </p>

  <a href="https://arxiv.org/abs/2203.03580">In a preliminary work presented at ICML</a>, we studied well-known pre-trained vision models
  in the context of control.
  Are supervised models better than self-supervised models?
  What kind of invariances are relevant for the perception module of the control policy?
  Is the feature hierarchy of the vision layers important for control?
  <br>
  By investigating these fundamental questions, we succeeded at making a single off-the-shelf vision model
  — trained on out-of-domain datasets — to be competitive with or even outperform ground-truth
  features on all the four control domains.
  As efficient compact state features are hard to estimate in unstructured real-world environments
  and the agent needs to rely on raw vision input,
  our model can be extremely beneficial by
  dramatically reducing the data requirement and improving the policy performance.

  <p><b>References </b></font></p>

  <p>
  <b> <a href="https://arxiv.org/abs/2203.03580">The (Un)Surprising Effectiveness of Pre-Trained Vision Models for Control</a> </b> <br>
  <font color=#f10000>Simone Parisi*</font>, Aravind Rajeswaran*, Senthil Purushwalkam, Abhinav Gupta <br>
  International Conference on Machine Learning (<u>ICML</u>), 2022
  </p>

  <hr>

  <p><b><font size=+1> Improving Actor-Critic Stability </font></b></p>
  <p>
  <img src="tdreg.png" width="60%" style="border-radius:0px; display: inline; float: left; margin: 0 11 0 0" >
  Actor-critic methods can achieve incredible performance but they are also prone to instability.
  This is partly due to the interaction between the
  actor and the critic during learning: an inaccurate step taken by one might adversely
  affect the other and destabilize the learning.
  </p>

  <p>
  <center><i>
  How can we make <u>any</u> actor-critic method more stable, especially when only few training samples are available?
  </i></center>
  </p>

  Our novel method, TD-regularized actor-critic (TD-REG), regularizes
  the actor loss by penalizing the TD-error of the
  critic. This improves stability by avoiding large steps in the actor update whenever the critic
  is highly inaccurate. TD-REG is a simple plug-and-play approach to improve stability and overall performance of
  any actor-critic method.


  <p><b>References </b></font></p>

  <p>
  <b> <a href="https://arxiv.org/abs/1812.08288">TD-Regularized Actor-Critic Methods</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Voot Tangkaratt, Jan Peters, Mohammad Emtiyaz Khan <br>
  Machine Learning, 2019
  </p>

  <hr>

  <p><b><font size=+1> RL Applied to Robotics: The Tetherball Platform </font></b></p>

  <p>
  <img src="tetherball.png" width="30%" style="border-radius:0px; display: inline; float: left; margin: 0 11 0 0" >
  Motor skills learning is an important challenge in order to endow robots with the ability
  to learn a wide range of skills and solve complex tasks.
  In the last decade, RL has been shown the ability to acquire a variety of skills,
  ranging from the game ball-in-a-cup to walking and jumping.
  </p>

  <p>
  <center><i>
  However, comparing RL against human programming	is not straightforward.
  RL policies try to optimize a given reward function, but using this reward function as comparison measure would introduce a bias,
  as there is no guarantee that the manual program maximizes the same reward function.
  </i></center>
  </p>

	To address the problem of finding a fair evaluation measure,
  <a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Member/PubSimoneParisi/parisi_iros_2015.pdf">we proposed a robotic task</a>
  based on the game of tetherball. In a game, in fact, there is a pre-defined success measure: the game score.
	The robotic platform consisted of two cable-driven lightweight robots capable of highly dynamic
  behavior due to springs between the motors and the cables which drive the joints.
	We manually programmed one robot player using the complete model of the tetherball game,
  while we trained the other player using RL.
	Evaluated on real games, the RL player outperformed the manually programmed one
  by winning more often.
  By learning by trial-and-error, in fact, the RL player could compensate for the prediction error
  of the highly nonlinear forward dynamics model, and errors in the ball tracking.


  <p><b>References </b></font></p>

  <p>
  <b> <a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Member/PubSimoneParisi/parisi_iros_2015.pdf">Reinforcement Learning vs Human Programming in Tetherball Robot Games</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Hany Abdulsamad, Alexandros Paraschos, Christian Daniel, Jan Peters <br>
  International Conference on Intelligent Robots and Systems (<u>IROS</u>), 2015
  </p>

  <hr>


  <p><b><font size=+1> Multi-Objective RL </font></b></p>
  <img src="morl_manifold.jpg" width="75%" style="border-radius:0px" >
  <br>

  Many real-world control applications are characterized
  by the presence of multiple conflicting objectives. In these problems,
  the goal is to find the Pareto frontier,
  a set of policies representing different compromises among the objectives.

  <p>
  <center><i>
  The Pareto frontier encapsulates all the trade-offs among the objectives and gives better insight
  into the problem, thus helping the a posteriori selection of the most favorable solution.
  How can we efficiently build an approximation of the frontier that contains
  solutions that are accurate, diverse, and evenly distributed?
  </i></center>
  </p>

  My research on this topic has focused on manifold-based methods that return
  a continuous approximation of the Pareto frontier: the idea is to optimize the parameters of
  a function defining a manifold in the policy parameters space, so that the corresponding
	image in the objectives space gets as close as possible to the true Pareto frontier.
  This allows to learn an approximation in a single pass instead of using multiple optimizations.
  <a href="https://jair.org/index.php/jair/article/view/11026">In a preliminary work</a>,
  we presented a gradient descent method and investigated its sample complexity and the effects of its hyperparameters.
  <a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Site/EditPublication/parisi_neurocomp_morl.pdf">Later</a>,
  we improved it by using episodic exploration strategies, importance sampling, and novel losses to assess the quality of
  a Pareto frontier approximation.


  <p><b>References </b></font></p>

  <p>
  <b> <a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Site/EditPublication/parisi_neurocomp_morl.pdf">Manifold-based Multi-objective Policy Search with Sample Reuse</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Matteo Pirotta, Jan Peters <br>
  Neurocomputing, 2017
  </p>

  <p>
  <b> <a href="https://jair.org/index.php/jair/article/view/11026">Multi-objective Reinforcement Learning through Continuous Pareto Manifold Approximation</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Matteo Pirotta, Marcello Restelli <br>
  Journal of Artificial Intelligence Research (<u>JAIR</u>), 2016
  </p>

  <p>
  <b> <a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Site/EditPublication/Parisi_IJCNN14.pdf">Policy Gradient Approaches for Multi-Objective Sequential Decision Making</a> </b> <br>
  <font color=#f10000>Simone Parisi</font>, Matteo Pirotta, Nicola Smacchia, Luca Bascetta, Marcello Restelli <br>
  International Joint Conference on Neural Networks (<u>IJCNN</u>), 2014
  </p>

  </td></tr>
  <hr>
  </table>


</body></html>
